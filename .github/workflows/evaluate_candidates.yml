name: Evaluate Algorithm Candidates

on:
  # Triggered when a new branch with the prefix 'evolve/' is pushed
  push:
    branches:
      - 'evolve/**'
  
  # Allow manual triggers for testing
  workflow_dispatch:
    inputs:
      branches:
        description: 'Comma-separated list of branches to evaluate'
        required: false
        default: ''

jobs:
  detect-branches:
    runs-on: ubuntu-latest
    outputs:
      branches: ${{ steps.set-branches.outputs.branches }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Set branches
        id: set-branches
        run: |
          if [[ -n "${{ github.event.inputs.branches }}" ]]; then
            # Use manually specified branches
            echo "branches=$(echo ${{ github.event.inputs.branches }} | jq -R 'split(",")')" >> $GITHUB_OUTPUT
          else
            # Auto-detect evolve branches
            echo "branches=$(git branch -r | grep 'origin/evolve/' | sed 's|origin/||' | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT
          fi

  evaluate:
    needs: detect-branches
    runs-on: ubuntu-latest
    strategy:
      matrix:
        branch: ${{ fromJson(needs.detect-branches.outputs.branches) }}
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Create worktree for branch
        run: |
          WORKTREE_PATH="../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')"
          git worktree add $WORKTREE_PATH ${{ matrix.branch }}
      
      - name: Run benchmarks
        run: |
          WORKTREE_PATH="../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')"
          cd $WORKTREE_PATH
          python benchmark.py --output-json benchmark_metrics.json
          python benchmark_metrics.py --component-level
      
      - name: Run tests
        run: |
          WORKTREE_PATH="../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')"
          cd $WORKTREE_PATH
          pytest test_model.py -v
          pytest test_benchmarks.py --benchmark-json pytest_benchmark.json
      
      - name: Generate benchmark report
        run: |
          WORKTREE_PATH="../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')"
          cd $WORKTREE_PATH
          python generate_benchmark_report.py
      
      - name: Upload metrics as artifact
        uses: actions/upload-artifact@v4
        with:
          name: metrics-${{ matrix.branch }}
          path: |
            ../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')/benchmark_metrics.json
            ../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')/component_benchmarks.json
            ../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')/pytest_benchmark.json
            ../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')/benchmark_report.md
      
      - name: Store score in git notes
        run: |
          WORKTREE_PATH="../candidate-$(echo ${{ matrix.branch }} | sed 's|/|-|g')"
          cd $WORKTREE_PATH
          SCORE=$(jq '.total_score' benchmark_metrics.json)
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git notes add -m "score:$SCORE" HEAD
          git push origin "refs/notes/*"

  # Third job: Compare all evaluated branches
  compare_branches:
    needs: evaluate
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas matplotlib tabulate
      
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: benchmark-results
      
      - name: Compare branches
        run: |
          # Create a script to combine and compare all benchmark results
          cat > compare_branches.py << 'EOF'
          import os
          import glob
          import json
          import pandas as pd
          import matplotlib.pyplot as plt
          
          def load_metrics_file(filepath):
              try:
                  with open(filepath, 'r') as f:
                      return json.load(f)
              except:
                  return None
          
          # Find all benchmark metrics files
          benchmark_dirs = glob.glob('benchmark-results/benchmark-*')
          branch_metrics = {}
          
          for benchmark_dir in benchmark_dirs:
              branch_name = benchmark_dir.split('-', 1)[1]
              metrics_file = os.path.join(benchmark_dir, 'benchmark_metrics.json')
              
              metrics = load_metrics_file(metrics_file)
              if metrics:
                  branch_metrics[branch_name] = {
                      'accuracy': metrics.get('accuracy', 0),
                      'training_time': metrics.get('training_time', 0),
                      'best_model': metrics.get('best_model', 'unknown')
                  }
          
          # Create comparison table
          if branch_metrics:
              df = pd.DataFrame.from_dict(branch_metrics, orient='index')
              print("\n=== Branch Comparison ===")
              print(df)
              
              # Save comparison to file
              with open('branch_comparison.md', 'w') as f:
                  f.write("# Algorithm Branch Comparison\n\n")
                  f.write(df.to_markdown())
                  
                  # Identify best branch
                  if 'accuracy' in df.columns:
                      best_branch = df['accuracy'].idxmax()
                      f.write(f"\n\n## Best Branch: {best_branch}\n")
                      f.write(f"- Accuracy: {df.loc[best_branch, 'accuracy']}\n")
                      f.write(f"- Training Time: {df.loc[best_branch, 'training_time']}\n")
                      f.write(f"- Best Model: {df.loc[best_branch, 'best_model']}\n")
              
              # Create comparison chart
              fig, axes = plt.subplots(1, 2, figsize=(12, 6))
              
              df['accuracy'].plot.bar(ax=axes[0], title='Accuracy Comparison')
              axes[0].set_ylim([df['accuracy'].min() * 0.95, min(1.0, df['accuracy'].max() * 1.05)])
              
              df['training_time'].plot.bar(ax=axes[1], title='Training Time Comparison')
              
              plt.tight_layout()
              plt.savefig('branch_comparison.png')
          else:
              print("No benchmark metrics found")
          EOF
          
          python compare_branches.py
      
      - name: Upload comparison results
        uses: actions/upload-artifact@v4
        with:
          name: branch-comparison
          path: |
            branch_comparison.md
            branch_comparison.png
      
      - name: Display best branch
        run: |
          echo "See branch comparison results in the artifacts"
          if [ -f branch_comparison.md ]; then
            cat branch_comparison.md
          fi 